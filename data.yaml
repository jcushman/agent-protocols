title: The Agent Protocol Tech Tree
subtitle: How open standards for AI agents emerge layer by layer
details: |
  Open protocols emerge when decentralized parties benefit from speaking the same language.
  If your software will be used more when it outputs data that other software can read,
  and reads data that other software produces, all tools tend to converge on a common format.
  AI agents are currently incentivized to converge in this way, and are rapidly exploring
  and adopting standards. This tool explores
  the standards they are converging on and why each layer came to be.


# Actor icon images — keyed by actor type, referenced in animations.
# Generate with: cd tools && uv run generate-images --patch-data
actor_icons:
  app: images/actor-app.png
  model: images/actor-model.png
  server: images/actor-server.png
  agent: images/actor-agent.png
  repo: images/actor-repo.png
  website: images/actor-website.png
  registry: images/actor-registry.png
  merchant: images/actor-merchant.png
  skill: images/actor-skill.png

# Clusters are decorative dashed-outline group boxes.
clusters:
  - id: c-foundations
    label: Model building standards
    members: [tokenizers, weight-formats, embeddings, chat-templates]

  - id: c-instruction
    label: Instruction Surfaces
    members: [agents-md, llms-txt, agent-skills]

  - id: c-domain
    label: Domain Protocols
    members: [ucp, agent-identity]

# ── Technologies ────────────────────────────────────────────────────
# Nodes use column-based layout: {parent, offset}.
# parent: id of parent node for Y positioning. offset: column offset from parent.
# unlock.state: locked | unlocked (default unlocked).
# unlock.class: foundational | frontier (determines overlay icon).

technologies:

  # ── Foundations (locked by default; stone-age unlocks) ─────────────
  - id: tokenizers
    title: Tokenizers
    icon: images/icon-tokenizers.png
    icon_alt: "tok"
    tagline: Portable text-to-token boundary
    layout: {}
    unlock: { state: locked, class: foundational }
    detail:
      links:
        - { label: "Hugging Face Tokenizers", url: "https://github.com/huggingface/tokenizers" }
        - { label: "SentencePiece", url: "https://github.com/google/sentencepiece" }
      what_it_solves: >-
        A portable text-to-token-id boundary so model weights and runtimes agree
        on exactly how to encode inputs and decode outputs.
      how_its_standardizing: >-
        Converged through de facto artifacts (vocab/merges, SentencePiece models,
        tokenizer.json) adopted by model publishers and runtimes rather than a
        formal standards body.
      virtuous_cycle:
        - "Model publishers ship a tokenizer artifact so downstream users can run the model."
        - "Runtime/tooling authors implement the common artifacts to support many models."
        - "New models choose widely supported tokenizer formats to maximize portability."

  - id: weight-formats
    title: Weight Formats
    icon: images/icon-weights.png
    icon_alt: "w"
    tagline: Safe model distribution
    layout: {}
    unlock: { state: locked, class: foundational }
    detail:
      links:
        - { label: "safetensors", url: "https://github.com/huggingface/safetensors" }
        - { label: "GGUF / llama.cpp", url: "https://github.com/ggerganov/llama.cpp" }
      what_it_solves: >-
        A safe, fast way to distribute model tensors across frameworks and runtimes.
      how_its_standardizing: >-
        De facto convergence on formats like safetensors (safety/perf) and GGUF
        (local inference ecosystem), driven by tool support and publishing norms.
      virtuous_cycle:
        - "Publishers pick formats that load everywhere with minimal risk."
        - "Runtimes support formats that unlock the largest model catalog."
        - "Converters/quantizers flourish once a stable on-disk format is common."

  - id: embeddings
    title: Embeddings
    icon: images/icon-embeddings.png
    icon_alt: "vec"
    tagline: Universal vector interface
    layout: {}
    unlock: { state: locked, class: foundational }
    detail:
      links:
        - { label: "Standard", url: "https://opensearch.org/docs/latest/search-plugins/knn/" }
      what_it_solves: >-
        A universal "vector + metadata" interface enabling retrieval, semantic search,
        and RAG pipelines regardless of the embedding model or vector store.
      how_its_standardizing: >-
        Converged by API similarity across vector DBs and common client expectations
        (float[] vectors, metadata filters, upsert/query) rather than a single spec.
      virtuous_cycle:
        - "Apps rely on a stable vector interface; swapping stores/models becomes easy."
        - "DBs adopt common ops (upsert/query/filter) to capture the same client ecosystem."
        - "Model providers target vector-friendly shapes to plug into common retrieval stacks."

  - id: chat-templates
    title: Chat Templates
    icon: images/icon-chat-templates.png
    icon_alt: "msg"
    tagline: Portable conversation format
    layout: {}
    unlock: { state: locked, class: foundational }
    detail:
      links:
        - { label: "Standard", url: "https://huggingface.co/docs/transformers/main/chat_templating" }
      what_it_solves: >-
        A portable way to represent multi-turn conversations and serialize them into
        model-specific token sequences correctly.
      how_its_standardizing: >-
        Dataset schemas converged on role/content turns; chat template metadata
        (e.g., in tokenizers/configs) enabled generic chat UIs to support many models.
      virtuous_cycle:
        - "Apps can target a consistent message schema across models."
        - "Model publishers ship templates so generic clients can format prompts correctly."
        - "Evaluation/training pipelines reuse the same role/content structure at scale."

  # ── Inference ─────────────────────────────────────────────────────
  - id: inference-api
    title: Inference API
    icon: images/icon-inference-api.png
    icon_alt: ">_"
    tagline: A stable way to talk to any model
    layout: { parents: [tokenizers, weight-formats, embeddings, chat-templates] }
    detail:
      links:
        - { label: "Standard", url: "https://platform.openai.com/docs/api-reference/chat" }
        - { label: "Ollama compatibility", url: "https://github.com/ollama/ollama/blob/main/docs/openai.md" }
        - { label: "vLLM compatibility", url: "https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html" }
      what_it_solves: >-
        Applications needed a consistent way to send prompts to language
        models and receive responses — without rewriting code for every
        provider. The chat completions API shape gave developers a single
        interface to target, making LLMs a reliable, swappable component.
      how_its_standardizing: >-
        OpenAI's Chat Completions API (March 2023) became the reference
        design for chat-shaped request/response. Many SDKs, gateways, and
        open-source servers (Ollama, vLLM, llama.cpp) expose OpenAI-compatible
        endpoints. Major providers like Anthropic and Google use different
        native APIs, but gateway/adapter tools provide compatibility layers.
        Aggregators like OpenRouter exposes hundreds of models behind the same API.
      virtuous_cycle:
        - "App makers target one interface and can switch providers with minimal changes."
        - "Model hosts gain adoption by being drop-in compatible with existing clients."
        - "Tooling (SDKs, gateways, proxies) compounds around the common shape."
    animation:
      actors:
        - id: app
          label: Your App
          type: app
        - id: openai
          label: OpenAI
          type: server
        - id: vllm
          label: vLLM Server
          type: server
        - id: ollama
          label: Ollama (Local)
          type: server
      scenes:
        - title: App talks to a model provider
          description: >-
            Your application sends a chat completion request to OpenAI
            using the standard message format.
          actors_visible: [app, openai]
          messages:
            - from: app
              to: openai
              label: Chat prompt
              json_preview: '{"model":"gpt-4","messages":[{"role":"user","content":"Hello"}]}'
              json_full: |
                POST /v1/chat/completions
                {
                  "model": "gpt-4",
                  "messages": [
                    {"role": "user", "content": "Hello, how are you?"}
                  ],
                  "temperature": 0.7,
                  "max_tokens": 256
                }
            - from: openai
              to: app
              label: Completion
              json_preview: '{"choices":[{"message":{"role":"assistant","content":"Hi there!"}}]}'
              json_full: |
                {
                  "id": "chatcmpl-abc123",
                  "object": "chat.completion",
                  "model": "gpt-4",
                  "choices": [{
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "Hi there! How can I help you today?"
                    },
                    "finish_reason": "stop"
                  }],
                  "usage": {
                    "prompt_tokens": 12,
                    "completion_tokens": 9,
                    "total_tokens": 21
                  }
                }
        - title: Same code, open-source server
          description: >-
            Switch to a vLLM server running open-weights models —
            the request and response shape stay the same.
          actors_visible: [app, vllm]
          sparkle: true
          messages:
            - from: app
              to: vllm
              label: Same format
              json_preview: '{"model":"meta-llama/Llama-3-70b","messages":[{"role":"user","content":"Hello"}]}'
            - from: vllm
              to: app
              label: Same shape response
              json_preview: '{"choices":[{"message":{"role":"assistant","content":"Hello!"}}]}'
        - title: Even works locally
          description: >-
            Ollama runs models on your own machine — and speaks the
            same API. One interface, every model.
          actors_visible: [app, ollama]
          sparkle: true
          messages:
            - from: app
              to: ollama
              label: Same format, locally
              json_preview: '{"model":"llama3","messages":[{"role":"user","content":"Hello"}]}'
            - from: ollama
              to: app
              label: Same shape response
              json_preview: '{"choices":[{"message":{"role":"assistant","content":"Hey there!"}}]}'

  # ── Tool Calling ──────────────────────────────────────────────────
  - id: tool-calling
    title: Tool Calling
    icon: images/icon-tool-calling.png
    icon_alt: "f(x)"
    tagline: Models that can take action
    layout: { col: 2, parent: inference-api }
    detail:
      links:
        - { label: "Launch", url: "https://openai.com/index/function-calling-and-other-api-updates/" }
        - { label: "Standard", url: "https://platform.openai.com/docs/guides/function-calling" }
      what_it_solves: >-
        Language models can only generate text — but applications need
        them to take real actions like searching the web, reading files,
        or calling APIs. Tool calling lets a model emit structured
        "call this function" requests, and then continue after the app
        executes them. Tool definitions use JSON Schema-like structures,
        making tools portable and machine-readable across providers.
      how_its_standardizing: >-
        OpenAI introduced "function calling" in June 2023, later
        generalized to "tool calling." Other providers rapidly adopted
        the same pattern. Standardization is de facto: JSON Schema-inspired
        signatures and tool-call/result message roles work across providers,
        though implementations vary slightly in supported schema features.
      virtuous_cycle:
        - "App makers expose capabilities once and reuse them across many models."
        - "Model makers support tool calling to be useful in real workflows, not just chat."
        - "Tool ecosystems emerge because definitions are machine-readable and reusable."
    animation:
      actors:
        - id: app
          label: Your App
          type: app
        - id: model
          label: LLM
          type: model
      scenes:
        - title: App provides tools to the model
          description: >-
            The app sends a prompt along with tool definitions
            describing what functions are available.
          actors_visible: [app, model]
          messages:
            - from: app
              to: model
              label: Prompt + tool definitions
              json_preview: '{"messages":[...],"tools":[{"type":"function","function":{"name":"search"}}]}'
              json_full: |
                POST /v1/chat/completions
                {
                  "model": "gpt-4",
                  "messages": [
                    {"role": "user", "content": "What's the weather in Boston?"}
                  ],
                  "tools": [{
                    "type": "function",
                    "function": {
                      "name": "get_weather",
                      "description": "Get current weather for a location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {"type": "string"}
                        },
                        "required": ["location"]
                      }
                    }
                  }]
                }
        - title: Model requests a tool call
          description: >-
            Instead of answering directly, the model emits a structured
            tool call — requesting the app to execute a function.
          actors_visible: [app, model]
          messages:
            - from: model
              to: app
              label: tool_call
              json_preview: '{"tool_calls":[{"function":{"name":"get_weather","arguments":"{\"location\":\"Boston\"}"}}]}'
              json_full: |
                {
                  "choices": [{
                    "message": {
                      "role": "assistant",
                      "tool_calls": [{
                        "id": "call_abc123",
                        "type": "function",
                        "function": {
                          "name": "get_weather",
                          "arguments": "{\"location\": \"Boston, MA\"}"
                        }
                      }]
                    },
                    "finish_reason": "tool_calls"
                  }]
                }
        - title: App executes and returns the result
          description: >-
            The app runs the function and sends the result back.
            The model incorporates it into its final answer.
          actors_visible: [app, model]
          messages:
            - from: app
              to: model
              label: Tool result
              json_preview: '{"role":"tool","content":"{\"temp\":\"62F\",\"condition\":\"Partly cloudy\"}"}'
              json_full: |
                {
                  "messages": [
                    {"role": "user", "content": "What's the weather in Boston?"},
                    {"role": "assistant", "tool_calls": [{"id": "call_abc123", "type": "function", "function": {"name": "get_weather", "arguments": "{\"location\": \"Boston, MA\"}"}}]},
                    {"role": "tool", "tool_call_id": "call_abc123", "content": "{\"temp\": \"62F\", \"condition\": \"Partly cloudy\", \"humidity\": \"45%\"}"}
                  ]
                }
            - from: model
              to: app
              label: Final answer
              json_preview: '{"content":"It''s 62F and partly cloudy in Boston."}'

  # ── MCP ───────────────────────────────────────────────────────────
  - id: mcp
    title: MCP
    icon: images/icon-mcp.png
    icon_alt: "<>"
    tagline: One protocol for every tool
    layout: { col: 3, parent: tool-calling }
    detail:
      links:
        - { label: "Launch", url: "https://www.anthropic.com/news/model-context-protocol" }
        - { label: "Spec (2025-03-26)", url: "https://modelcontextprotocol.io/specification/2025-03-26" }
        - { label: "AAIF governance", url: "https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation" }
      what_it_solves: >-
        Without MCP, connecting N apps to M tool providers requires N*M
        custom integrations. MCP defines a single client/server protocol
        so any app can discover and use any tool server — tools/list to
        find what's available, tools/call to use it.
      how_its_standardizing: >-
        Introduced by Anthropic (November 2024). Rapidly adopted across
        the agent tooling ecosystem. Now under open governance through
        the Agentic AI Foundation (AAIF) at the Linux Foundation, with
        community contributions and a growing ecosystem of client and
        server implementations.
      virtuous_cycle:
        - "Tool providers implement one server and reach many MCP clients."
        - "Clients add one protocol and gain access to a growing tool catalog."
        - "Shared conventions reduce integration cost and accelerate new tools."
    animation:
      actors:
        - id: app
          label: Claude
          type: app
        - id: file-server
          label: File Server
          type: server
        - id: db-server
          label: DB Server
          type: server
        - id: cursor
          label: Cursor
          type: app
      scenes:
        - title: Client discovers server tools
          description: >-
            An MCP client connects to a file server and asks what
            tools are available using the standard tools/list method.
          actors_visible: [app, file-server]
          messages:
            - from: app
              to: file-server
              label: tools/list
              json_preview: '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'
              json_full: |
                {
                  "jsonrpc": "2.0",
                  "id": 1,
                  "method": "tools/list",
                  "params": {}
                }
            - from: file-server
              to: app
              label: Available tools
              json_preview: '{"result":[{"name":"read_file","description":"Read a file"}]}'
              json_full: |
                {
                  "jsonrpc": "2.0",
                  "id": 1,
                  "result": [{
                    "name": "read_file",
                    "description": "Read a file from the filesystem",
                    "inputSchema": {
                      "type": "object",
                      "properties": {
                        "path": {"type": "string", "description": "File path to read"}
                      },
                      "required": ["path"]
                    }
                  }]
                }
        - title: Client calls a tool
          description: >-
            The client invokes a tool on the server using tools/call.
            The server executes it and returns the result.
          actors_visible: [app, file-server]
          messages:
            - from: app
              to: file-server
              label: tools/call
              json_preview: '{"method":"tools/call","params":{"name":"read_file","arguments":{"path":"README.md"}}}'
              json_full: |
                {
                  "jsonrpc": "2.0",
                  "id": 2,
                  "method": "tools/call",
                  "params": {
                    "name": "read_file",
                    "arguments": {"path": "README.md"}
                  }
                }
            - from: file-server
              to: app
              label: Tool result
              json_preview: '{"result":{"content":[{"type":"text","text":"# My Project..."}]}}'
        - title: Same client, different server
          description: >-
            The same MCP client connects to a database server.
            Same protocol — tools/list, tools/call — different tools.
          actors_visible: [app, db-server]
          sparkle: true
          messages:
            - from: app
              to: db-server
              label: tools/list
              json_preview: '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'
            - from: db-server
              to: app
              label: Available tools
              json_preview: '{"result":[{"name":"query","description":"Run a SQL query"}]}'
        - title: Different client, same server
          description: >-
            A completely different app — Cursor — connects to the
            same file server. No changes needed on either side.
          actors_visible: [cursor, file-server]
          sparkle: true
          messages:
            - from: cursor
              to: file-server
              label: tools/list
              json_preview: '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'
            - from: file-server
              to: cursor
              label: Same tools
              json_preview: '{"result":[{"name":"read_file","description":"Read a file"}]}'

  # ── MCP Apps ──────────────────────────────────────────────────────
  - id: mcp-apps
    title: MCP Apps
    icon: images/icon-mcp-apps.png
    icon_alt: "UI"
    tagline: Interactive UIs for tool servers
    layout: { col: 4, parent: mcp }
    detail:
      links:
        - { label: "Extension docs", url: "https://modelcontextprotocol.io/docs/extensions/apps" }
        - { label: "MCP repo", url: "https://github.com/modelcontextprotocol" }
      what_it_solves: >-
        Lets tool providers ship sandboxed UI surfaces (previews, editors,
        dashboards) integrated into agent hosts — without giving raw OS
        control to the model.
      how_its_standardizing: >-
        Evolved as an MCP extension. Standardization is driven by host
        support (clients) and shared UI embedding + messaging conventions
        (sandboxed iframe + notifications).
      virtuous_cycle:
        - "Tool providers can deliver better UX without per-host bespoke UI work."
        - "Hosts gain safer, richer interactions than pure JSON tool results."
        - "A common UI embedding model encourages more reusable tool apps."
    animation:
      actors:
        - id: agent
          label: Agent Host
          type: app
        - id: tool
          label: Tool Server
          type: server
        - id: app
          label: MCP App UI
          type: app
      scenes:
        - title: Tool returns a UI surface
          description: >-
            Instead of raw JSON, the tool advertises an interactive UI
            surface that can be embedded by the host.
          actors_visible: [agent, tool]
          messages:
            - from: agent
              to: tool
              label: tools/call
              json_preview: '{"name":"inspect","arguments":{"id":"123"}}'
            - from: tool
              to: agent
              label: app descriptor
              json_preview: '{"type":"app","src":"https://tool/ui/inspect"}'
        - title: Host embeds the UI
          description: >-
            The agent host renders the UI in a sandboxed iframe and
            communicates with it via MCP messaging.
          actors_visible: [agent, app]
          sparkle: true
          messages:
            - from: agent
              to: app
              label: embed
              json_preview: '{"sandbox":true}'

  # ── Standard Tool Surfaces ────────────────────────────────────────
  - id: standard-tool-surfaces
    title: Standard Tools
    icon: images/icon-tool-surfaces.png
    icon_alt: "cli"
    tagline: Browser, Command Line, and Code
    layout: { col: 3, parent: tool-calling }
    detail:
      links:
        - { label: "Standard", url: "https://platform.openai.com/docs/guides/tools" }
      what_it_solves: >-
        A small set of canonical action surfaces that tool ecosystems and
        finetunes converge on: browser, command line, and code
        execution/editing. These cover most real-world agent tasks.
      how_its_standardizing: >-
        Not a single spec; a convergence in agent platforms toward stable
        tool categories and semantics (navigate/click/type; run commands;
        apply patches/tests) because those cover most real-world tasks.
      virtuous_cycle:
        - "Tool makers target the common surfaces to reach many agent hosts."
        - "Agent hosts standardize these surfaces to maximize reliability and safety controls."
        - "Model training/evaluation can focus on a stable set of action types."
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: browser
          label: Browser Tool
          type: server
        - id: shell
          label: Command Tool
          type: server
      scenes:
        - title: Browser interaction
          description: >-
            The agent navigates a website using a standard browser action
            vocabulary: navigate, click, type.
          actors_visible: [agent, browser]
          messages:
            - from: agent
              to: browser
              label: navigate
              json_preview: '{"url":"https://example.com"}'
            - from: browser
              to: agent
              label: page loaded
              json_preview: '{"title":"Example Domain"}'
        - title: Command execution
          description: >-
            The same agent switches to a shell tool using a familiar
            run/exit/stdout pattern.
          actors_visible: [agent, shell]
          sparkle: true
          messages:
            - from: agent
              to: shell
              label: run
              json_preview: '{"cmd":"pytest"}'
            - from: shell
              to: agent
              label: result
              json_preview: '{"exit_code":0}'

  # ── Instruction Surfaces ──────────────────────────────────────────
  - id: agents-md
    title: AGENTS.md
    icon: images/icon-agents-md.png
    icon_alt: "#"
    tagline: Project-local guidance for agents
    layout: { parent: tool-calling, offset: 2 }
    detail:
      links:
        - { label: "Standard", url: "https://agents.md/" }
      what_it_solves: >-
        Coding agents drop into unfamiliar repositories and need to know:
        how do I build this? What style conventions matter? What should I
        never touch? AGENTS.md gives them a predictable, standardized
        place to find project-specific instructions.
      how_its_standardizing: >-
        Community-driven format that emerged from the practice of
        embedding agent instructions in READMEs. Formalized with a
        public spec and conventions for sections. Reported adoption
        across 60,000+ projects per agents.md tracking.
      virtuous_cycle:
        - "Repo maintainers write instructions once for all agent tools."
        - "Agent tools auto-discover guidance, reducing failures and friction."
        - "A shared convention encourages better repo hygiene for automation."
    animation:
      actors:
        - id: agent
          label: Coding Agent
          type: agent
        - id: agent-b
          label: Other Agent
          type: agent
        - id: repo-a
          label: Project Alpha
          type: repo
        - id: repo-b
          label: Project Beta
          type: repo
      scenes:
        - title: Agent opens a repo
          description: >-
            A coding agent is dropped into a repository. It looks for
            AGENTS.md at the project root.
          actors_visible: [agent, repo-a]
          messages:
            - from: agent
              to: repo-a
              label: Read AGENTS.md
              json_preview: 'GET /AGENTS.md'
            - from: repo-a
              to: agent
              label: Project instructions
              json_preview: '## Setup\n- `uv sync && uv run pytest`\n## Style\n- Run ruff format'
              json_full: |
                # AGENTS.md

                ## Setup
                - `uv sync`
                - `uv run pytest`

                ## How to work
                - Prefer small PRs.
                - Run `ruff format` before commit.

                ## Boundaries
                - Never change prod credentials handling without a reviewer.
        - title: Agent follows the instructions
          description: >-
            The agent reads the setup, style, and boundary sections,
            then works within those constraints automatically.
          actors_visible: [agent, repo-a]
          messages:
            - from: agent
              to: repo-a
              label: Runs setup
              json_preview: '$ uv sync && uv run pytest'
            - from: agent
              to: repo-a
              label: Follows style
              json_preview: '$ ruff format src/'
        - title: Different repo, same convention
          description: >-
            The agent moves to a completely different project. It finds
            another AGENTS.md and adapts immediately — no reconfiguration.
          actors_visible: [agent, repo-b]
          sparkle: true
          messages:
            - from: agent
              to: repo-b
              label: Read AGENTS.md
              json_preview: 'GET /AGENTS.md'
            - from: repo-b
              to: agent
              label: Different instructions
              json_preview: '## Setup\n- `npm install && npm test`\n## Style\n- Use TypeScript strict'
        - title: Different agent, same repo
          description: >-
            A completely different coding agent opens the same project.
            Same AGENTS.md, same instructions — write once, any agent reads.
          actors_visible: [agent-b, repo-a]
          sparkle: true
          messages:
            - from: agent-b
              to: repo-a
              label: Read AGENTS.md
              json_preview: 'GET /AGENTS.md'
            - from: repo-a
              to: agent-b
              label: Same instructions
              json_preview: '## Setup\n- `uv sync && uv run pytest`\n## Style\n- Run ruff format'

  - id: llms-txt
    title: llms.txt
    icon: images/icon-llms-txt.png
    icon_alt: "//"
    tagline: AI-readable site index
    layout: { parent: tool-calling, offset: 2 }
    detail:
      links:
        - { label: "Standard", url: "https://llmstxt.org/" }
      what_it_solves: >-
        LLMs and agents need to find the right documentation on a website
        efficiently. Sitemaps are designed for search engine crawlers, not
        AI. llms.txt is a simple Markdown index at a well-known URL that
        tells agents exactly where to look.
      how_its_standardizing: >-
        Proposed by Jeremy Howard (September 2024). Adopted by
        documentation platforms and developer tools, with growing
        ecosystem support for auto-generating llms.txt files from
        existing site content.
      virtuous_cycle:
        - "Sites publish an index; agents spend fewer tokens and fetch fewer pages."
        - "Docs platforms add generators; adoption becomes near-zero effort."
        - "Agent tooling becomes more reliable across arbitrary sites."
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: site-a
          label: docs.example.com
          type: website
        - id: site-b
          label: api.other.dev
          type: website
      scenes:
        - title: Agent needs documentation
          description: >-
            An agent needs to understand an API. Instead of crawling
            the whole site, it fetches /llms.txt for a curated index.
          actors_visible: [agent, site-a]
          messages:
            - from: agent
              to: site-a
              label: Fetch /llms.txt
              json_preview: 'GET /llms.txt'
            - from: site-a
              to: agent
              label: Structured index
              json_preview: '# docs.example.com\n## Docs\n- /docs/getting-started\n- /docs/api'
              json_full: |
                # docs.example.com

                ## Docs
                - /docs/getting-started
                - /docs/api

                ## Canonical specs
                - /docs/protocol
                - /docs/security

                ## Examples
                - /docs/examples/basic
        - title: Agent fetches exactly what it needs
          description: >-
            With the index, the agent knows precisely which pages to
            read — no wasted tokens on irrelevant content.
          actors_visible: [agent, site-a]
          messages:
            - from: agent
              to: site-a
              label: Fetch specific page
              json_preview: 'GET /docs/api'
            - from: site-a
              to: agent
              label: Focused content
              json_preview: '# API Reference\n## Authentication\nUse Bearer tokens...'
        - title: Works on any site
          description: >-
            A different website also publishes llms.txt. The agent
            uses the same approach — one convention, every site.
          actors_visible: [agent, site-b]
          sparkle: true
          messages:
            - from: agent
              to: site-b
              label: Fetch /llms.txt
              json_preview: 'GET /llms.txt'
            - from: site-b
              to: agent
              label: Structured index
              json_preview: '# api.other.dev\n## Endpoints\n- /reference/auth\n- /reference/users'

  - id: agent-skills
    title: Agent Skills
    icon: images/icon-agent-skills.png
    icon_alt: "pkg"
    tagline: Portable capability packages
    layout: { parent: tool-calling, offset: 2 }
    detail:
      links:
        - { label: "Standard", url: "https://agentskills.io/specification" }
        - { label: "Codex skills docs", url: "https://developers.openai.com/codex/skills/" }
      what_it_solves: >-
        Agents need reusable playbooks — task-specific knowledge with
        optional scripts and references. Agent Skills package this know-how
        in a standard directory format that any agent platform can discover
        and use.
      how_its_standardizing: >-
        Open standard with emerging adoption among IDE agents. OpenAI's
        Codex docs explicitly state it "builds on the open agent skills
        standard." Companies like Cloudflare publish skills repos for
        cross-platform use.
      virtuous_cycle:
        - "Skill authors write once and reach many agent hosts."
        - "Hosts get a growing library of reusable procedures."
        - "Skills become a distribution channel for best practices."
    animation:
      actors:
        - id: skill
          label: Weather Skill
          type: skill
        - id: agent-a
          label: Claude
          type: agent
        - id: agent-b
          label: Cursor
          type: agent
      scenes:
        - title: A skill is published
          description: >-
            A developer creates a skill directory with SKILL.md
            describing what the skill does and how to use it.
          actors_visible: [agent-a, skill]
          messages:
            - from: agent-a
              to: skill
              label: Discover skill
              json_preview: 'Read skills/weather/SKILL.md'
            - from: skill
              to: agent-a
              label: Skill definition
              json_preview: '# Weather lookup\n## Inputs\n- location (string)\n## Usage\nCall scripts/get_weather.py'
              json_full: |
                skills/weather/
                  SKILL.md
                  scripts/get_weather.py
                  references/open-meteo.md

                # Weather lookup

                ## Inputs
                - location (string)

                ## Output
                - JSON: { "temp_c": number, "summary": string }

                ## Usage
                Call `scripts/get_weather.py --location "Boston, MA"`.
        - title: Agent uses the skill
          description: >-
            The agent follows the skill instructions — reading the
            playbook, running scripts, consulting references.
          actors_visible: [agent-a, skill]
          messages:
            - from: agent-a
              to: skill
              label: Execute
              json_preview: '$ python scripts/get_weather.py --location "Boston, MA"'
            - from: skill
              to: agent-a
              label: Result
              json_preview: '{"temp_c": 17, "summary": "Partly cloudy"}'
        - title: Same skill, different agent
          description: >-
            A completely different agent platform loads the same skill.
            The standard directory format means it just works.
          actors_visible: [agent-b, skill]
          sparkle: true
          messages:
            - from: agent-b
              to: skill
              label: Discover skill
              json_preview: 'Read skills/weather/SKILL.md'
            - from: skill
              to: agent-b
              label: Same definition
              json_preview: '# Weather lookup\n## Inputs\n- location (string)'


  - id: skill-security
    title: Skill Signing
    icon: images/icon-skill-security.png
    icon_alt: "lock"
    tagline: Safe distribution of capabilities
    layout: { col: 5, parent: agent-skills }
    unlock: { state: locked, class: frontier }
    detail:
      links:
        - { label: "Standard (inspiration)", url: "https://slsa.dev/" }
      what_it_solves: >-
        Prevents malicious or over-privileged skills by standardizing
        signing/provenance, declared permissions, sandbox expectations,
        and review metadata.
      how_its_standardizing: >-
        Likely to borrow from supply-chain frameworks (provenance + signing)
        and converge within agent skill ecosystems as marketplaces mature.
      virtuous_cycle:
        - "Users can trust skills based on provenance + declared permissions."
        - "Marketplaces reduce risk without bespoke vetting per host."
        - "Skill authors get a portable security envelope across platforms."
    animation:
      actors:
        - id: agent
          label: Agent Host
          type: agent
        - id: registry
          label: Skill Registry
          type: registry
      scenes:
        - title: Verify skill provenance
          description: >-
            Before loading a skill, the agent host verifies its signature
            and declared permissions.
          actors_visible: [agent, registry]
          messages:
            - from: agent
              to: registry
              label: fetch metadata
              json_preview: '{"skill":"weather","version":"1.2.0"}'
            - from: registry
              to: agent
              label: signed manifest
              json_preview: '{"signature":"...","permissions":["network"]}'


  # ── Agent Loop ──────────────────────────────────────────────────────
  - id: agent-loop
    title: Agent Loop
    icon: images/icon-agent-loop.png
    icon_alt: "loop"
    tagline: Models that keep going
    layout: { col: 3, parent: tool-calling }
    detail:
      links:
        - { label: "Pattern", url: "https://www.anthropic.com/engineering/building-effective-agents" }
      what_it_solves: >-
        Tool calling lets a model request one action — but real tasks require
        many steps. The agent loop is the pattern of repeatedly calling the
        model, executing its tool requests, feeding results back, and continuing
        until the model signals completion. This loop turns a chat model into
        an autonomous agent.
      how_its_standardizing: >-
        De facto convergence across agent frameworks (LangChain, AutoGPT,
        Claude computer use, OpenAI Assistants). The pattern is consistent:
        system prompt + user goal, model responds with tool calls or final
        answer, host executes and re-prompts. No formal spec, but the shape
        is universal.
      virtuous_cycle:
        - "Developers build on the pattern rather than inventing custom control flows."
        - "Models are trained to emit stop signals and multi-step plans reliably."
        - "Tooling (tracing, guardrails, timeouts) targets a predictable execution model."
    animation:
      actors:
        - id: agent
          label: Agent Host
          type: agent
        - id: model
          label: Model
          type: model
        - id: tool
          label: Tool
          type: server
      scenes:
        - title: Plan
          description: >-
            The agent asks the model what to do next given the current state.
          actors_visible: [agent, model]
          messages:
            - from: agent
              to: model
              label: prompt
              json_preview: '{"goal":"Fix failing tests"}'
        - title: Act
          description: >-
            The model emits a tool call; the host executes it.
          actors_visible: [agent, model, tool]
          messages:
            - from: model
              to: agent
              label: tool_call
              json_preview: '{"name":"run_tests"}'
            - from: agent
              to: tool
              label: execute
              json_preview: '{"cmd":"pytest"}'
        - title: Iterate
          description: >-
            Results are fed back into the model, continuing the loop
            until completion.
          actors_visible: [agent, model]
          sparkle: true
          messages:
            - from: agent
              to: model
              label: result
              json_preview: '{"failures":0}'

  # ── A2A ───────────────────────────────────────────────────────────
  - id: a2a
    title: A2A
    icon: images/icon-a2a.png
    icon_alt: "<>"
    tagline: Agents working across frameworks
    layout: { col: 4, parent: agent-loop }
    detail:
      links:
        - { label: "Launch", url: "https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/" }
        - { label: "Specification", url: "https://agent2agent.info/specification/" }
        - { label: "Linux Foundation", url: "https://www.linuxfoundation.org/projects/agent2agent" }
      what_it_solves: >-
        Agents built on different frameworks — by different vendors — need
        to discover each other, negotiate capabilities, and coordinate on
        tasks. A2A provides a standard protocol for agent-to-agent
        interoperability without requiring shared internals.
      how_its_standardizing: >-
        Announced by Google (April 2025). Quickly moved to open
        governance under the Linux Foundation. Implemented by multiple
        platforms; Microsoft ecosystem documentation positions it as a
        standard integration surface.
      virtuous_cycle:
        - "Enterprises can compose agents across vendors instead of buying a monolith."
        - "Vendors become more valuable by being interoperable, not isolated."
        - "A shared task/artifact model enables tooling (routers, registries, monitors)."
    animation:
      actors:
        - id: client
          label: Your Agent
          type: agent
        - id: invoice-agent
          label: Invoice Agent
          type: agent
        - id: research-agent
          label: Research Agent
          type: agent
      scenes:
        - title: Discover a remote agent
          description: >-
            Your agent fetches the remote agent's AgentCard — a JSON
            file at a well-known URL describing capabilities and skills.
          actors_visible: [client, invoice-agent]
          messages:
            - from: client
              to: invoice-agent
              label: Fetch AgentCard
              json_preview: 'GET /.well-known/agent-card.json'
            - from: invoice-agent
              to: client
              label: Capabilities
              json_preview: '{"name":"InvoiceAgent","skills":[{"id":"extract","name":"Extract invoice"}]}'
              json_full: |
                {
                  "name": "InvoiceAgent",
                  "description": "Extracts invoice fields from PDFs",
                  "url": "https://agent.example/a2a",
                  "provider": {
                    "organization": "ExampleCo",
                    "url": "https://example.com"
                  },
                  "version": "1.0.0",
                  "capabilities": {
                    "streaming": true,
                    "pushNotifications": false,
                    "stateTransitionHistory": true
                  },
                  "authentication": {"schemes": ["Bearer"]},
                  "defaultInputModes": ["application/pdf", "text/plain"],
                  "defaultOutputModes": ["application/json"],
                  "skills": [{
                    "id": "extract",
                    "name": "Extract invoice",
                    "description": "Parse invoice fields from documents",
                    "tags": ["ap"],
                    "examples": ["Extract totals from this PDF"]
                  }]
                }
        - title: Send a task
          description: >-
            Your agent sends a task with messages. The remote agent
            processes it and returns artifacts.
          actors_visible: [client, invoice-agent]
          messages:
            - from: client
              to: invoice-agent
              label: Submit task
              json_preview: '{"task":{"id":"t-123","state":"submitted"},"messages":[{"role":"user","parts":[...]}]}'
              json_full: |
                {
                  "task": {
                    "id": "t-123",
                    "state": "submitted"
                  },
                  "messages": [{
                    "role": "user",
                    "parts": [{
                      "content_type": "text/plain",
                      "content": "Extract invoice fields from https://example.com/inv.pdf"
                    }]
                  }]
                }
            - from: invoice-agent
              to: client
              label: Task artifacts
              json_preview: '{"task":{"state":"completed"},"artifacts":[{"parts":[{"content":"{...}"}]}]}'
        - title: Different agent, same protocol
          description: >-
            Your agent discovers a Research Agent at a different
            organization. Same AgentCard format, same task protocol.
          actors_visible: [client, research-agent]
          sparkle: true
          messages:
            - from: client
              to: research-agent
              label: Fetch AgentCard
              json_preview: 'GET /.well-known/agent-card.json'
            - from: research-agent
              to: client
              label: Capabilities
              json_preview: '{"name":"ResearchAgent","skills":[{"id":"search","name":"Deep research"}]}'

  # ── Domain Protocols ──────────────────────────────────────────────
  - id: ucp
    title: UCP
    icon: images/icon-ucp.png
    icon_alt: "$"
    tagline: Universal agent commerce
    layout: { col: 4, parent: a2a }
    detail:
      links:
        - { label: "Shopify launch", url: "https://www.shopify.com/ucp" }
        - { label: "Google explainer", url: "https://developers.googleblog.com/under-the-hood-universal-commerce-protocol-ucp/" }
      what_it_solves: >-
        Agents that shop need a standard way to browse products, build
        carts, and complete checkout across different merchants — without
        custom integration per store. UCP defines capabilities discovery,
        checkout sessions, and payment handlers for agent commerce.
      how_its_standardizing: >-
        Launched January 2026 by Shopify and Google. Specifies capability
        negotiation, checkout session creation, and payment handler
        interfaces. Supported by large retailers and payments ecosystem
        partners. Open for any merchant, agent, or payment processor.
      virtuous_cycle:
        - "Merchants implement once and accept many agents."
        - "Agents implement once and can shop broadly."
        - "Payments/logistics partners standardize integrations around a common flow."
    animation:
      actors:
        - id: agent
          label: Shopping Agent
          type: agent
        - id: merchant-a
          label: Flower Shop
          type: merchant
        - id: merchant-b
          label: Book Store
          type: merchant
      scenes:
        - title: Agent creates a checkout
          description: >-
            The shopping agent creates a checkout session with a
            merchant, sending line items and buyer info.
          actors_visible: [agent, merchant-a]
          messages:
            - from: agent
              to: merchant-a
              label: Create checkout session
              json_preview: '{"line_items":[{"item":{"id":"bouquet_roses"},"quantity":1}],"currency":"USD"}'
              json_full: |
                POST /checkout-sessions
                UCP-Agent: profile="https://agent.example/profile"
                {
                  "line_items": [{
                    "item": {"id": "bouquet_roses", "title": "Red Rose Bouquet"},
                    "quantity": 1
                  }],
                  "buyer": {
                    "full_name": "John Doe",
                    "email": "john.doe@example.com"
                  },
                  "currency": "USD",
                  "payment": {
                    "instruments": [],
                    "handlers": [{
                      "id": "shop_pay",
                      "name": "com.shopify.shop_pay",
                      "version": "2026-01-11"
                    }]
                  }
                }
            - from: merchant-a
              to: agent
              label: Session ready
              json_preview: '{"id":"session-abc","status":"ready_for_complete","total":"$49.99"}'
              json_full: |
                {
                  "ucp": {
                    "version": "2026-01-11",
                    "capabilities": [{
                      "name": "dev.ucp.shopping.checkout",
                      "version": "2026-01-11"
                    }]
                  },
                  "id": "session-abc",
                  "status": "ready_for_complete",
                  "total": {"amount": 4999, "currency": "USD"}
                }
        - title: Agent completes purchase
          description: >-
            The agent confirms the purchase. The merchant processes
            payment and returns an order confirmation.
          actors_visible: [agent, merchant-a]
          messages:
            - from: agent
              to: merchant-a
              label: Complete checkout
              json_preview: '{"session_id":"session-abc","action":"complete"}'
            - from: merchant-a
              to: agent
              label: Order confirmed
              json_preview: '{"order_id":"ord-789","status":"confirmed"}'
        - title: Same protocol, different merchant
          description: >-
            The agent shops at a completely different store. Same
            checkout flow, same API shape — no new integration needed.
          actors_visible: [agent, merchant-b]
          sparkle: true
          messages:
            - from: agent
              to: merchant-b
              label: Create checkout session
              json_preview: '{"line_items":[{"item":{"id":"novel_123"},"quantity":2}],"currency":"USD"}'
            - from: merchant-b
              to: agent
              label: Session ready
              json_preview: '{"id":"session-xyz","status":"ready_for_complete","total":"$31.98"}'

  - id: agent-identity
    title: Agent Identity
    icon: images/icon-agent-identity.png
    icon_alt: "id"
    tagline: Verified agents on the open web
    layout: { col: 4, parent: a2a }
    detail:
      links:
        - { label: "HTTP Message Signatures (IETF)", url: "https://datatracker.ietf.org/wg/httpbis/documents/" }
        - { label: "Cloudflare Web Bot Auth", url: "https://blog.cloudflare.com/secure-agentic-commerce/" }
        - { label: "Visa TAP", url: "https://developer.visa.com/capabilities/trusted-agent-protocol/trusted-agent-protocol-specifications" }
      what_it_solves: >-
        Websites and merchants need to distinguish legitimate,
        user-authorized agents from scrapers and bots. Agent Identity
        protocols use cryptographic signatures to prove an agent's
        identity on every request — no CAPTCHAs, no IP allowlists.
      how_its_standardizing: >-
        Built on HTTP Message Signatures (IETF standards track) as the
        base primitive. Domain profiles add agent-specific semantics:
        Cloudflare's Web Bot Auth and Visa's Trusted Agent Protocol (TAP)
        both use the same signature foundation with different registry
        and tagging conventions for their ecosystems.
      virtuous_cycle:
        - "Sites can apply policy based on verified identity rather than guesswork."
        - "Agents get reliable access without adversarial bot mitigation."
        - "Commerce and high-trust domains get a shared trust substrate."
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: merchant
          label: Merchant Site
          type: merchant
        - id: registry
          label: Key Registry
          type: registry
      scenes:
        - title: Agent signs its request
          description: >-
            The agent makes an HTTP request and attaches a
            cryptographic signature using HTTP Message Signatures.
          actors_visible: [agent, merchant]
          messages:
            - from: agent
              to: merchant
              label: Signed request
              json_preview: 'Signature-Input: sig2=("@authority" "@path");keyid="...";tag="agent-browser-auth"'
              json_full: |
                GET /example-product HTTP/1.1
                Host: example.com
                Signature-Input: sig2=("@authority" "@path");created=1735689600;
                  expires=1735693200;keyid="agent-key-123";
                  alg="Ed25519";nonce="abc123";
                  tag="agent-browser-auth"
                Signature: sig2=:BASE64_SIGNATURE_HERE:
        - title: Merchant verifies identity
          description: >-
            The merchant retrieves the agent's public key and verifies
            the signature. Legitimate agents get through; unknown bots don't.
          actors_visible: [agent, merchant, registry]
          messages:
            - from: merchant
              to: registry
              label: Fetch public key
              json_preview: 'GET /.well-known/agent-keys/agent-key-123'
            - from: registry
              to: merchant
              label: Public key
              json_preview: '{"keyid":"agent-key-123","alg":"Ed25519","public_key":"..."}'
            - from: merchant
              to: agent
              label: Access granted
              json_preview: 'HTTP/1.1 200 OK'
        - title: Same signatures work everywhere
          description: >-
            Whether it's Visa TAP, Cloudflare Web Bot Auth, or
            Mastercard Agent Pay — the same HTTP Message Signatures
            standard works across all of them.
          actors_visible: [agent, merchant]
          sparkle: true
          messages:
            - from: agent
              to: merchant
              label: 'Signed: tag="web-bot-auth"'
              json_preview: 'Signature-Input: sig2=("@authority" "@path");tag="web-bot-auth"'
            - from: merchant
              to: agent
              label: Verified
              json_preview: 'HTTP/1.1 200 OK — Agent identity confirmed'

  # ── Frontier (locked by default; rocket unlocks) ──────────────────
  - id: agent-audit
    title: Agent Audit Traces
    icon: images/icon-audit.png
    icon_alt: "log"
    tagline: Portable run records
    layout: { parent: agent-loop }
    unlock: { state: locked, class: frontier }
    detail:
      links:
        - { label: "Standard (substrate)", url: "https://opentelemetry.io/docs/concepts/signals/traces/" }
      what_it_solves: >-
        A standard event+artifact record of what an agent did (model calls,
        tool calls, browser actions, file writes) for replay, debugging,
        and incident response.
      how_its_standardizing: >-
        Likely to layer on OpenTelemetry-like traces with agent-specific
        event schemas, redaction, and artifact references.
      virtuous_cycle:
        - "Operators can debug and govern agents across vendors with one trace format."
        - "Vendors integrate once with monitoring and compliance tooling."
        - "Evaluations and safety reviews become reproducible and comparable."
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: trace
          label: Trace Store
          type: server
      scenes:
        - title: Emit trace events
          description: >-
            Each model call and tool action is recorded as a structured
            trace event.
          actors_visible: [agent, trace]
          messages:
            - from: agent
              to: trace
              label: span
              json_preview: '{"event":"tool_call","name":"read_file"}'

  - id: ap2
    title: AP2
    icon: images/icon-ap2.png
    icon_alt: "pay"
    tagline: Agent-to-agent payments
    layout: { col: 5, parent: agent-identity }
    detail:
      links:
        - { label: "Specification", url: "https://ap2-protocol.org/specification/" }
      what_it_solves: >-
        Agents that transact on behalf of users need a standard way to
        request, authorize, and settle payments — with cryptographic
        proof of user consent and spending limits. AP2 defines payment
        mandates that bind user intent to agent actions.
      how_its_standardizing: >-
        Open specification for agent-initiated payments. Defines mandate
        objects with scope, limits, and TTL that travel with payment
        requests. Designed to work alongside identity protocols like
        TAP and Web Bot Auth.
      virtuous_cycle:
        - "Agents can transact across merchants with verifiable authorization."
        - "Merchants get cryptographic proof of user consent, reducing fraud."
        - "Payment rails gain a standard agent interface instead of per-agent integrations."
    animation:
      actors:
        - id: agent
          label: Paying Agent
          type: agent
        - id: merchant
          label: Merchant
          type: merchant
        - id: wallet
          label: Wallet
          type: server
      scenes:
        - title: Present payment mandate
          description: >-
            The agent submits a signed mandate proving user authorization
            and spending limits.
          actors_visible: [agent, merchant]
          messages:
            - from: agent
              to: merchant
              label: pay
              json_preview: '{"mandate_id":"m-123","amount":4999}'
        - title: Settle payment
          description: >-
            The merchant verifies the mandate and settles through a
            compatible payment rail.
          actors_visible: [merchant, wallet]
          sparkle: true
          messages:
            - from: merchant
              to: wallet
              label: charge
              json_preview: '{"mandate_id":"m-123"}'

  - id: intent-mandates
    title: Signed Intent Mandates
    icon: images/icon-mandate.png
    icon_alt: "sig"
    tagline: Verifiable consent for actions
    layout: { col: 6, parent: ap2 }
    unlock: { state: locked, class: frontier }
    detail:
      links:
        - { label: "Inspiration (AP2)", url: "https://ap2-protocol.org/specification/" }
      what_it_solves: >-
        A signed object binding user intent and constraints (scope, limits,
        TTL) to downstream agent actions — generalizing the "payments
        mandate" pattern from AP2 to arbitrary domains.
      how_its_standardizing: >-
        Expected to converge first in regulated domains (payments, IT ops,
        healthcare), then generalize to cross-domain consent receipts with
        standard fields. AP2's payment mandates provide the reference design.
      virtuous_cycle:
        - "Counterparties can rely on machine-verifiable authorization."
        - "Users get enforceable constraints instead of trusting opaque agents."
        - "Platforms interoperate on the same consent artifact across workflows."
    animation:
      actors:
        - id: user
          label: User
          type: app
        - id: agent
          label: Agent
          type: agent
        - id: service
          label: Service
          type: server
      scenes:
        - title: User signs intent
          description: >-
            The user signs an intent object defining scope, limits,
            and expiration.
          actors_visible: [user, agent]
          messages:
            - from: user
              to: agent
              label: sign
              json_preview: '{"action":"deploy","ttl":"1h"}'
        - title: Agent presents mandate
          description: >-
            The agent attaches the signed mandate to downstream requests.
          actors_visible: [agent, service]
          sparkle: true
          messages:
            - from: agent
              to: service
              label: authorized request
              json_preview: '{"mandate":"signed_blob"}'

  - id: agent-card
    title: AgentCard
    icon: images/icon-agent-card.png
    icon_alt: "card"
    tagline: Well-known agent descriptors
    layout: { parent: agent-loop }
    detail:
      links:
        - { label: "A2A AgentCard spec", url: "https://agent2agent.info/specification/#agentcard" }
      what_it_solves: >-
        Before agents can collaborate, they need to discover each other's
        capabilities, endpoints, and authentication requirements. AgentCard
        is a JSON descriptor at a well-known URL that tells other agents
        what this agent can do and how to talk to it.
      how_its_standardizing: >-
        Defined as part of A2A. Agents publish a JSON file at
        /.well-known/agent-card.json describing name, skills, supported
        input/output modes, and auth schemes. Provides the discovery
        primitive that higher-level registries can index.
      virtuous_cycle:
        - "Agents can discover each other without a central directory."
        - "Registries index the same descriptor format across vendors."
        - "A standard metadata shape enables tooling (routers, monitors, catalogs)."
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: remote
          label: Remote Agent
          type: agent
      scenes:
        - title: Discover capabilities
          description: >-
            An agent fetches a remote agent's well-known AgentCard
            descriptor.
          actors_visible: [agent, remote]
          messages:
            - from: agent
              to: remote
              label: GET /.well-known/agent-card.json
              json_preview: '{}'
            - from: remote
              to: agent
              label: AgentCard
              json_preview: '{"skills":["search","summarize"]}'

  - id: agent-registries
    title: Agent Registries
    icon: images/icon-registry.png
    icon_alt: "dir"
    tagline: Discovery, routing, permissions
    layout: { parent: agent-card }
    unlock: { state: locked, class: frontier }
    detail:
      links:
        - { label: "Draft (example)", url: "https://datatracker.ietf.org/doc/draft-narvaneni-agent-uri/" }
      what_it_solves: >-
        A standard directory for agents: capability queries, routing,
        health/versioning, and enterprise entitlements.
      how_its_standardizing: >-
        Likely to emerge from enterprise catalogs and public indexes
        that aggregate AgentCard descriptors. A2A's well-known discovery
        provides the base layer; registries add search, ACLs, and routing.
      virtuous_cycle:
        - "Enterprises can safely compose multi-vendor agent systems."
        - "Vendors plug into one directory surface instead of bespoke integrations."
        - "Governance (who can call what) becomes portable."
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: registry
          label: Registry
          type: registry
      scenes:
        - title: Query registry
          description: >-
            An agent searches a registry for other agents matching
            required capabilities.
          actors_visible: [agent, registry]
          messages:
            - from: agent
              to: registry
              label: search
              json_preview: '{"skill":"invoice-extraction"}'
            - from: registry
              to: agent
              label: results
              json_preview: '[{"agent":"InvoiceAgent"}]'

  - id: agent-lingua
    title: Agent Lingua Franca
    icon: images/icon-lingua.png
    icon_alt: "abc"
    tagline: Token-efficient inter-agent speech
    layout: { parent: agent-loop }
    unlock: { state: locked, class: frontier }
    detail:
      links:
        - { label: "Research (inspiration)", url: "https://arxiv.org/abs/2310.06562" }
      what_it_solves: >-
        When agents communicate, they use verbose natural language or structured
        JSON — neither optimized for inter-agent bandwidth. A shared compressed
        vocabulary or symbolic language could dramatically reduce token costs
        while preserving semantics across different models and tokenizers.
      how_its_standardizing: >-
        Expected to emerge as multi-agent orchestration scales and costs matter.
        Could layer compression codebooks on A2A, develop learned shorthand via
        multi-agent RL, or converge on symbolic representations that transcend
        natural language tokenization boundaries.
      virtuous_cycle:
        - "Multi-agent workflows become economical as token overhead shrinks."
        - "Heterogeneous model teams communicate efficiently despite different tokenizers."
        - "Shared vocabularies enable richer coordination primitives than natural language."
    animation:
      actors:
        - id: agent-a
          label: Agent A
          type: agent
        - id: agent-b
          label: Agent B
          type: agent
      scenes:
        - title: Compressed exchange
          description: >-
            Agents communicate using a shared compressed representation
            instead of natural language.
          actors_visible: [agent-a, agent-b]
          sparkle: true
          messages:
            - from: agent-a
              to: agent-b
              label: message
              json_preview: '{"sym":"Δ7α"}'

  - id: human-context-protocol
    title: Human Context Protocol
    icon: images/icon-hcp.png
    icon_alt: "ctx"
    tagline: Portable user memory and preferences
    layout: { parent: agent-loop }
    unlock: { state: locked, class: frontier }
    detail:
      links:
        - { label: "Project", url: "https://digitaleconomy.stanford.edu/project/loyal-agents/hcp-human-context-protocol/" }
        - { label: "Paper", url: "https://digitaleconomy.stanford.edu/wp-content/uploads/2024/11/Loyal-Agents.pdf" }
      what_it_solves: >-
        Agents today manage user preferences, goals, and long-term context in
        ad hoc, vendor-specific memory systems. This makes it impossible for users
        to switch agents, combine agents, or delegate tasks across systems without
        losing intent, preferences, and consent. HCP defines a clean boundary
        between an agent and a user-aligned memory manager.
      how_its_standardizing: >-
        HCP is a research proposal from the Stanford Digital Economy Lab,
        not yet a formal specification. It proposes a conceptual protocol
        surface — schema definition, preference search, and preference update —
        between agents and external "human context" stores. Explicitly framed
        to enable cross-vendor interoperability and user-controlled memory
        portability if adopted.
      virtuous_cycle:
        - "Users gain portable, agent-agnostic preferences and long-term context."
        - "Agent builders can offload memory, consent, and personalization safely."
        - "Memory providers can specialize and compete without locking in agents."
    animation:
      actors:
        - id: agent
          label: AI Agent
          type: agent
        - id: memory-a
          label: Context Manager
          type: server
        - id: memory-b
          label: Other Provider
          type: server
      scenes:
        - title: Define preference schema
          description: >-
            The agent declares the structure of user preferences it may read or
            write, without embedding them directly in prompts.
          actors_visible: [agent, memory-a]
          messages:
            - from: agent
              to: memory-a
              label: Define schema
              json_preview: '{"namespace":"prefs","fields":["tone","units"]}'
        - title: Query human context
          description: >-
            The agent retrieves relevant preferences at runtime, scoped to the
            current task.
          actors_visible: [agent, memory-a]
          messages:
            - from: agent
              to: memory-a
              label: Search prefs
              json_preview: '{"query":"units","top_k":1}'
            - from: memory-a
              to: agent
              label: Preference
              json_preview: '{"key":"units","value":"metric"}'
        - title: Swap memory provider
          description: >-
            A different HCP-compliant context manager can be used without changing
            the agent logic — user preferences travel with the user.
          actors_visible: [agent, memory-b]
          sparkle: true
          messages:
            - from: agent
              to: memory-b
              label: Search prefs
              json_preview: '{"query":"units","top_k":1}'
            - from: memory-b
              to: agent
              label: Same interface
              json_preview: '{"key":"units","value":"metric"}'
