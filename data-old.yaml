title: The Agent Protocol Tech Tree
subtitle: How open standards for AI agents emerged, layer by layer

# Actor icon images — keyed by actor type, referenced in animations.
# Generate with: cd tools && uv run generate-images --patch-data
actor_icons:
  app: images/actor-app.png
  model: images/actor-model.png
  server: images/actor-server.png
  agent: images/actor-agent.png
  repo: images/actor-repo.png
  website: images/actor-website.png
  registry: images/actor-registry.png
  merchant: images/actor-merchant.png
  skill: images/actor-skill.png

eras:
  - col: 0
    label: Inference
  - col: 1
    label: Tool Use
  - col: 2
    label: Schemas
  - col: 3
    label: Ecosystem
  - col: 4
    label: Domains

groups:
  - label: Instruction Surfaces
    members: [agents-md, llms-txt, agent-skills]
  - label: Domain Protocols
    members: [ucp, agent-identity]

protocols:
  # ── Layer 1: Inference ─────────────────────────────────────────
  - id: inference-api
    title: Inference API
    icon: images/icon-inference-api.png
    icon_alt: ">_"
    tagline: A stable way to talk to any model
    tree:
      col: 0
      row: 2
      depends_on: []
    detail:
      what_it_solves: >-
        Applications needed a consistent way to send prompts to language
        models and receive responses — without rewriting code for every
        provider. The chat completions API shape gave developers a single
        interface to target, making LLMs a reliable, swappable component.
      why_open: >-
        Allows developers to switch between models — or use several at
        once — with the same code. No vendor lock-in at the API layer.
      where_from: >-
        OpenAI's Chat Completions API (March 2023) became the de facto
        standard. Competitors adopted compatible endpoints rather than
        fragmenting the ecosystem.
      who_maintains: >-
        No single standards body. OpenAI originated the shape; Anthropic,
        Google, Mistral, and open-source servers (Ollama, vLLM, llama.cpp)
        all maintain compatible implementations.
    animation:
      actors:
        - id: app
          label: Your App
          type: app
        - id: openai
          label: OpenAI
          type: server
        - id: anthropic
          label: Anthropic
          type: server
        - id: ollama
          label: Ollama (Local)
          type: server
      scenes:
        - title: App talks to a model provider
          description: >-
            Your application sends a chat completion request to OpenAI
            using the standard message format.
          actors_visible: [app, openai]
          messages:
            - from: app
              to: openai
              label: Chat prompt
              json_preview: '{"model":"gpt-4","messages":[{"role":"user","content":"Hello"}]}'
              json_full: |
                POST /v1/chat/completions
                {
                  "model": "gpt-4",
                  "messages": [
                    {"role": "user", "content": "Hello, how are you?"}
                  ],
                  "temperature": 0.7,
                  "max_tokens": 256
                }
            - from: openai
              to: app
              label: Completion
              json_preview: '{"choices":[{"message":{"role":"assistant","content":"Hi there!"}}]}'
              json_full: |
                {
                  "id": "chatcmpl-abc123",
                  "object": "chat.completion",
                  "model": "gpt-4",
                  "choices": [{
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "Hi there! How can I help you today?"
                    },
                    "finish_reason": "stop"
                  }],
                  "usage": {
                    "prompt_tokens": 12,
                    "completion_tokens": 9,
                    "total_tokens": 21
                  }
                }
        - title: Same code, different provider
          description: >-
            Switch to Anthropic — the request and response shape
            stay the same. No code changes needed.
          actors_visible: [app, anthropic]
          sparkle: true
          messages:
            - from: app
              to: anthropic
              label: Same format
              json_preview: '{"model":"claude-sonnet-4-20250514","messages":[{"role":"user","content":"Hello"}]}'
            - from: anthropic
              to: app
              label: Same shape response
              json_preview: '{"choices":[{"message":{"role":"assistant","content":"Hello!"}}]}'
        - title: Even works locally
          description: >-
            Ollama runs models on your own machine — and speaks the
            same API. One interface, every model.
          actors_visible: [app, ollama]
          sparkle: true
          messages:
            - from: app
              to: ollama
              label: Same format, locally
              json_preview: '{"model":"llama3","messages":[{"role":"user","content":"Hello"}]}'
            - from: ollama
              to: app
              label: Same shape response
              json_preview: '{"choices":[{"message":{"role":"assistant","content":"Hey there!"}}]}'

  # ── Layer 2: Tool Use ──────────────────────────────────────────
  - id: tool-calling
    title: Tool Calling
    icon: images/icon-tool-calling.png
    icon_alt: "f(x)"
    tagline: Models that can take action
    tree:
      col: 1
      row: 2
      depends_on: [inference-api]
    detail:
      what_it_solves: >-
        Language models can only generate text — but applications need
        them to take real actions like searching the web, reading files,
        or calling APIs. Tool calling lets a model emit structured
        "call this function" requests, and then continue after the app
        executes them.
      why_open: >-
        A shared tool-calling convention means any tool definition works
        with any model that supports the pattern — no per-model custom
        training required.
      where_from: >-
        OpenAI introduced "function calling" in June 2023, later
        generalized to "tool calling." Other providers rapidly adopted
        the same pattern.
      who_maintains: >-
        OpenAI originated the convention. Anthropic, Google, Mistral,
        and open-source model servers all implement compatible versions.
    animation:
      actors:
        - id: app
          label: Your App
          type: app
        - id: model
          label: LLM
          type: model
      scenes:
        - title: App provides tools to the model
          description: >-
            The app sends a prompt along with tool definitions
            describing what functions are available.
          actors_visible: [app, model]
          messages:
            - from: app
              to: model
              label: Prompt + tool definitions
              json_preview: '{"messages":[...],"tools":[{"type":"function","function":{"name":"search"}}]}'
              json_full: |
                POST /v1/chat/completions
                {
                  "model": "gpt-4",
                  "messages": [
                    {"role": "user", "content": "What's the weather in Boston?"}
                  ],
                  "tools": [{
                    "type": "function",
                    "function": {
                      "name": "get_weather",
                      "description": "Get current weather for a location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {"type": "string"}
                        },
                        "required": ["location"]
                      }
                    }
                  }]
                }
        - title: Model requests a tool call
          description: >-
            Instead of answering directly, the model emits a structured
            tool call — requesting the app to execute a function.
          actors_visible: [app, model]
          messages:
            - from: model
              to: app
              label: tool_call
              json_preview: '{"tool_calls":[{"function":{"name":"get_weather","arguments":"{\"location\":\"Boston\"}"}}]}'
              json_full: |
                {
                  "choices": [{
                    "message": {
                      "role": "assistant",
                      "tool_calls": [{
                        "id": "call_abc123",
                        "type": "function",
                        "function": {
                          "name": "get_weather",
                          "arguments": "{\"location\": \"Boston, MA\"}"
                        }
                      }]
                    },
                    "finish_reason": "tool_calls"
                  }]
                }
        - title: App executes and returns the result
          description: >-
            The app runs the function and sends the result back.
            The model incorporates it into its final answer.
          actors_visible: [app, model]
          messages:
            - from: app
              to: model
              label: Tool result
              json_preview: '{"role":"tool","content":"{\"temp\":\"62°F\",\"condition\":\"Partly cloudy\"}"}'
              json_full: |
                {
                  "messages": [
                    {"role": "user", "content": "What's the weather in Boston?"},
                    {"role": "assistant", "tool_calls": [{"id": "call_abc123", "type": "function", "function": {"name": "get_weather", "arguments": "{\"location\": \"Boston, MA\"}"}}]},
                    {"role": "tool", "tool_call_id": "call_abc123", "content": "{\"temp\": \"62°F\", \"condition\": \"Partly cloudy\", \"humidity\": \"45%\"}"}
                  ]
                }
            - from: model
              to: app
              label: Final answer
              json_preview: '{"content":"It''s 62°F and partly cloudy in Boston."}'

  # ── Layer 3: Schemas ───────────────────────────────────────────
  - id: tool-schemas
    title: Tool Schemas
    icon: images/icon-tool-schemas.png
    icon_alt: "{}"
    tagline: Machine-readable tool contracts
    tree:
      col: 2
      row: 2
      depends_on: [tool-calling]
    detail:
      what_it_solves: >-
        For tool calling to scale, tools need machine-readable descriptions
        so any model can understand what's available, what arguments are
        expected, and what comes back. Standard schemas make tools portable
        and discoverable.
      why_open: >-
        A shared description format means any tool can be used by any agent
        without custom glue code. Build a tool once, describe it in OpenAPI
        or JSON Schema, and every agent can use it.
      where_from: >-
        Built on OpenAPI (originally Swagger, 2011) and JSON Schema.
        OpenAI's plugin manifest (2023) codified the pattern for AI use
        with /.well-known/ai-plugin.json pointing to an OpenAPI spec.
      who_maintains: >-
        OpenAPI Initiative (Linux Foundation). JSON Schema is a separate
        community standard. The AI-specific conventions are de facto
        patterns across providers.
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: weather-api
          label: Weather API
          type: server
        - id: stock-api
          label: Stock API
          type: server
      scenes:
        - title: An API publishes its schema
          description: >-
            The Weather API publishes an OpenAPI spec describing its
            endpoints, parameters, and response formats.
          actors_visible: [agent, weather-api]
          messages:
            - from: agent
              to: weather-api
              label: Fetch schema
              json_preview: 'GET /.well-known/ai-plugin.json'
            - from: weather-api
              to: agent
              label: OpenAPI spec
              json_preview: '{"paths":{"/weather":{"get":{"parameters":[{"name":"location"}]}}}}'
              json_full: |
                {
                  "schema_version": "v1",
                  "name_for_model": "weather_api",
                  "api": {
                    "type": "openapi",
                    "url": "https://weather.example/openapi.yaml"
                  },
                  "description_for_model": "Get current weather for any location."
                }
        - title: Agent calls the API using the schema
          description: >-
            The agent reads the schema and knows exactly how to call
            the API — no custom integration needed.
          actors_visible: [agent, weather-api]
          messages:
            - from: agent
              to: weather-api
              label: Structured call
              json_preview: 'GET /weather?location=Boston'
            - from: weather-api
              to: agent
              label: Structured response
              json_preview: '{"temp":"62°F","condition":"Partly cloudy"}'
        - title: Same format, different API
          description: >-
            A Stock API also publishes an OpenAPI spec. The agent can
            use it immediately — same description format, zero new code.
          actors_visible: [agent, stock-api]
          sparkle: true
          messages:
            - from: agent
              to: stock-api
              label: Fetch schema
              json_preview: 'GET /.well-known/ai-plugin.json'
            - from: stock-api
              to: agent
              label: OpenAPI spec
              json_preview: '{"paths":{"/quote":{"get":{"parameters":[{"name":"symbol"}]}}}}'

  # ── Layer 4: Tool Connectivity ─────────────────────────────────
  - id: mcp
    title: MCP
    icon: images/icon-mcp.png
    icon_alt: "<>"
    tagline: One protocol for every tool
    tree:
      col: 3
      row: 0
      depends_on: [tool-schemas]
    detail:
      what_it_solves: >-
        Without MCP, connecting N apps to M tool providers requires N×M
        custom integrations. MCP defines a single client/server protocol
        so any app can discover and use any tool server — tools/list to
        find what's available, tools/call to use it.
      why_open: >-
        Any developer can build an MCP server, and it works with Claude,
        VS Code, Cursor, and every other MCP client. Build once, connect
        everywhere.
      where_from: >-
        Introduced by Anthropic (November 2024). Rapidly adopted across
        the agent tooling ecosystem.
      who_maintains: >-
        Anthropic initiated the spec. Open governance is expanding, with
        community contributions and a growing ecosystem of client and
        server implementations.
    animation:
      actors:
        - id: app
          label: Claude
          type: app
        - id: file-server
          label: File Server
          type: server
        - id: db-server
          label: DB Server
          type: server
        - id: cursor
          label: Cursor
          type: app
      scenes:
        - title: Client discovers server tools
          description: >-
            An MCP client connects to a file server and asks what
            tools are available using the standard tools/list method.
          actors_visible: [app, file-server]
          messages:
            - from: app
              to: file-server
              label: tools/list
              json_preview: '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'
              json_full: |
                {
                  "jsonrpc": "2.0",
                  "id": 1,
                  "method": "tools/list",
                  "params": {}
                }
            - from: file-server
              to: app
              label: Available tools
              json_preview: '{"result":[{"name":"read_file","description":"Read a file"}]}'
              json_full: |
                {
                  "jsonrpc": "2.0",
                  "id": 1,
                  "result": [{
                    "name": "read_file",
                    "description": "Read a file from the filesystem",
                    "inputSchema": {
                      "type": "object",
                      "properties": {
                        "path": {"type": "string", "description": "File path to read"}
                      },
                      "required": ["path"]
                    }
                  }]
                }
        - title: Client calls a tool
          description: >-
            The client invokes a tool on the server using tools/call.
            The server executes it and returns the result.
          actors_visible: [app, file-server]
          messages:
            - from: app
              to: file-server
              label: tools/call
              json_preview: '{"method":"tools/call","params":{"name":"read_file","arguments":{"path":"README.md"}}}'
              json_full: |
                {
                  "jsonrpc": "2.0",
                  "id": 2,
                  "method": "tools/call",
                  "params": {
                    "name": "read_file",
                    "arguments": {"path": "README.md"}
                  }
                }
            - from: file-server
              to: app
              label: Tool result
              json_preview: '{"result":{"content":[{"type":"text","text":"# My Project..."}]}}'
        - title: Same client, different server
          description: >-
            The same MCP client connects to a database server.
            Same protocol — tools/list, tools/call — different tools.
          actors_visible: [app, db-server]
          sparkle: true
          messages:
            - from: app
              to: db-server
              label: tools/list
              json_preview: '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'
            - from: db-server
              to: app
              label: Available tools
              json_preview: '{"result":[{"name":"query","description":"Run a SQL query"}]}'
        - title: Different client, same server
          description: >-
            A completely different app — Cursor — connects to the
            same file server. No changes needed on either side.
          actors_visible: [cursor, file-server]
          sparkle: true
          messages:
            - from: cursor
              to: file-server
              label: tools/list
              json_preview: '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'
            - from: file-server
              to: cursor
              label: Same tools
              json_preview: '{"result":[{"name":"read_file","description":"Read a file"}]}'

  # ── Layer 5: Instruction Surfaces ──────────────────────────────
  - id: agents-md
    title: AGENTS.md
    icon: images/icon-agents-md.png
    icon_alt: "#"
    tagline: Project-local guidance for agents
    tree:
      col: 3
      row: 1
      depends_on: [tool-schemas]
    detail:
      what_it_solves: >-
        Coding agents drop into unfamiliar repositories and need to know:
        how do I build this? What style conventions matter? What should I
        never touch? AGENTS.md gives them a predictable, standardized
        place to find project-specific instructions.
      why_open: >-
        Any repo can add one, any agent can read it. No platform lock-in —
        the same file works whether you're using Claude, Copilot, Cursor,
        or Codex.
      where_from: >-
        Community-driven format that emerged from the practice of
        embedding agent instructions in READMEs. Formalized with a
        public spec and conventions for sections.
      who_maintains: >-
        Community standard with a public spec site and GitHub
        organization. GitHub has published guidance on writing
        effective AGENTS.md files.
    animation:
      actors:
        - id: agent
          label: Coding Agent
          type: agent
        - id: repo-a
          label: Project Alpha
          type: repo
        - id: repo-b
          label: Project Beta
          type: repo
      scenes:
        - title: Agent opens a repo
          description: >-
            A coding agent is dropped into a repository. It looks for
            AGENTS.md at the project root.
          actors_visible: [agent, repo-a]
          messages:
            - from: agent
              to: repo-a
              label: Read AGENTS.md
              json_preview: 'GET /AGENTS.md'
            - from: repo-a
              to: agent
              label: Project instructions
              json_preview: '## Setup\n- `uv sync && uv run pytest`\n## Style\n- Run ruff format'
              json_full: |
                # AGENTS.md

                ## Setup
                - `uv sync`
                - `uv run pytest`

                ## How to work
                - Prefer small PRs.
                - Run `ruff format` before commit.

                ## Boundaries
                - Never change prod credentials handling without a reviewer.
        - title: Agent follows the instructions
          description: >-
            The agent reads the setup, style, and boundary sections,
            then works within those constraints automatically.
          actors_visible: [agent, repo-a]
          messages:
            - from: agent
              to: repo-a
              label: Runs setup
              json_preview: '$ uv sync && uv run pytest'
            - from: agent
              to: repo-a
              label: Follows style
              json_preview: '$ ruff format src/'
        - title: Different repo, same convention
          description: >-
            The agent moves to a completely different project. It finds
            another AGENTS.md and adapts immediately — no reconfiguration.
          actors_visible: [agent, repo-b]
          sparkle: true
          messages:
            - from: agent
              to: repo-b
              label: Read AGENTS.md
              json_preview: 'GET /AGENTS.md'
            - from: repo-b
              to: agent
              label: Different instructions
              json_preview: '## Setup\n- `npm install && npm test`\n## Style\n- Use TypeScript strict'

  - id: llms-txt
    title: llms.txt
    icon: images/icon-llms-txt.png
    icon_alt: "//"
    tagline: AI-readable site index
    tree:
      col: 3
      row: 2
      depends_on: [tool-schemas]
    detail:
      what_it_solves: >-
        LLMs and agents need to find the right documentation on a website
        efficiently. Sitemaps are designed for search engine crawlers, not
        AI. llms.txt is a simple Markdown index at a well-known URL that
        tells agents exactly where to look.
      why_open: >-
        Any website can publish one, any LLM or agent can read it.
        It's a plain Markdown file — no special tooling, APIs, or
        platform dependencies required.
      where_from: >-
        Proposed by Jeremy Howard (September 2024). Quickly adopted
        by documentation platforms and developer tools.
      who_maintains: >-
        Community standard. Documentation platforms like Mintlify and
        ReadMe have added first-class support for auto-generating
        llms.txt files.
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: site-a
          label: docs.example.com
          type: website
        - id: site-b
          label: api.other.dev
          type: website
      scenes:
        - title: Agent needs documentation
          description: >-
            An agent needs to understand an API. Instead of crawling
            the whole site, it fetches /llms.txt for a curated index.
          actors_visible: [agent, site-a]
          messages:
            - from: agent
              to: site-a
              label: Fetch /llms.txt
              json_preview: 'GET /llms.txt'
            - from: site-a
              to: agent
              label: Structured index
              json_preview: '# docs.example.com\n## Docs\n- /docs/getting-started\n- /docs/api'
              json_full: |
                # docs.example.com

                ## Docs
                - /docs/getting-started
                - /docs/api

                ## Canonical specs
                - /docs/protocol
                - /docs/security

                ## Examples
                - /docs/examples/basic
        - title: Agent fetches exactly what it needs
          description: >-
            With the index, the agent knows precisely which pages to
            read — no wasted tokens on irrelevant content.
          actors_visible: [agent, site-a]
          messages:
            - from: agent
              to: site-a
              label: Fetch specific page
              json_preview: 'GET /docs/api'
            - from: site-a
              to: agent
              label: Focused content
              json_preview: '# API Reference\n## Authentication\nUse Bearer tokens...'
        - title: Works on any site
          description: >-
            A different website also publishes llms.txt. The agent
            uses the same approach — one convention, every site.
          actors_visible: [agent, site-b]
          sparkle: true
          messages:
            - from: agent
              to: site-b
              label: Fetch /llms.txt
              json_preview: 'GET /llms.txt'
            - from: site-b
              to: agent
              label: Structured index
              json_preview: '# api.other.dev\n## Endpoints\n- /reference/auth\n- /reference/users'

  - id: agent-skills
    title: Agent Skills
    icon: images/icon-agent-skills.png
    icon_alt: "pkg"
    tagline: Portable capability packages
    tree:
      col: 3
      row: 3
      depends_on: [tool-schemas]
    detail:
      what_it_solves: >-
        Agents need reusable playbooks — task-specific knowledge with
        optional scripts and references. Agent Skills package this know-how
        in a standard directory format that any agent platform can discover
        and use.
      why_open: >-
        Write a skill once, use it in Claude, Copilot, Cursor, Codex, or
        any compatible agent. Skills are portable across platforms without
        modification.
      where_from: >-
        Open standard with multiple ecosystem participants. OpenAI's Codex
        docs explicitly state it "builds on the open agent skills standard."
      who_maintains: >-
        Community standard. Implemented by VS Code Copilot, Cursor,
        OpenAI Codex. Companies like Cloudflare publish skills repos.
    animation:
      actors:
        - id: skill
          label: Weather Skill
          type: skill
        - id: agent-a
          label: Claude
          type: agent
        - id: agent-b
          label: Cursor
          type: agent
      scenes:
        - title: A skill is published
          description: >-
            A developer creates a skill directory with SKILL.md
            describing what the skill does and how to use it.
          actors_visible: [agent-a, skill]
          messages:
            - from: agent-a
              to: skill
              label: Discover skill
              json_preview: 'Read skills/weather/SKILL.md'
            - from: skill
              to: agent-a
              label: Skill definition
              json_preview: '# Weather lookup\n## Inputs\n- location (string)\n## Usage\nCall scripts/get_weather.py'
              json_full: |
                skills/weather/
                  SKILL.md
                  scripts/get_weather.py
                  references/open-meteo.md

                # Weather lookup

                ## Inputs
                - location (string)

                ## Output
                - JSON: { "temp_c": number, "summary": string }

                ## Usage
                Call `scripts/get_weather.py --location "Boston, MA"`.
        - title: Agent uses the skill
          description: >-
            The agent follows the skill instructions — reading the
            playbook, running scripts, consulting references.
          actors_visible: [agent-a, skill]
          messages:
            - from: agent-a
              to: skill
              label: Execute
              json_preview: '$ python scripts/get_weather.py --location "Boston, MA"'
            - from: skill
              to: agent-a
              label: Result
              json_preview: '{"temp_c": 17, "summary": "Partly cloudy"}'
        - title: Same skill, different agent
          description: >-
            A completely different agent platform loads the same skill.
            The standard directory format means it just works.
          actors_visible: [agent-b, skill]
          sparkle: true
          messages:
            - from: agent-b
              to: skill
              label: Discover skill
              json_preview: 'Read skills/weather/SKILL.md'
            - from: skill
              to: agent-b
              label: Same definition
              json_preview: '# Weather lookup\n## Inputs\n- location (string)'

  # ── Layer 6: Agent-to-Agent ────────────────────────────────────
  - id: a2a
    title: A2A
    icon: images/icon-a2a.png
    icon_alt: "<>"
    tagline: Agents working across frameworks
    tree:
      col: 3
      row: 4
      depends_on: [tool-schemas]
    detail:
      what_it_solves: >-
        Agents built on different frameworks — by different vendors — need
        to discover each other, negotiate capabilities, and coordinate on
        tasks. A2A provides a standard protocol for agent-to-agent
        interoperability without requiring shared internals.
      why_open: >-
        Lets enterprises compose agent systems across vendors without
        lock-in. Any framework can implement A2A, and any A2A agent can
        work with any other.
      where_from: >-
        Announced by Google (April 2025). Quickly moved to open
        governance under the Linux Foundation.
      who_maintains: >-
        Linux Foundation project. Implemented by multiple platforms;
        Microsoft ecosystem documentation positions it as a standard
        integration surface.
    animation:
      actors:
        - id: client
          label: Your Agent
          type: agent
        - id: invoice-agent
          label: Invoice Agent
          type: agent
        - id: research-agent
          label: Research Agent
          type: agent
      scenes:
        - title: Discover a remote agent
          description: >-
            Your agent fetches the remote agent's AgentCard — a JSON
            file at a well-known URL describing capabilities and skills.
          actors_visible: [client, invoice-agent]
          messages:
            - from: client
              to: invoice-agent
              label: Fetch AgentCard
              json_preview: 'GET /.well-known/agent-card.json'
            - from: invoice-agent
              to: client
              label: Capabilities
              json_preview: '{"name":"InvoiceAgent","skills":[{"id":"extract","name":"Extract invoice"}]}'
              json_full: |
                {
                  "name": "InvoiceAgent",
                  "description": "Extracts invoice fields from PDFs",
                  "url": "https://agent.example/a2a",
                  "provider": {
                    "organization": "ExampleCo",
                    "url": "https://example.com"
                  },
                  "version": "1.0.0",
                  "capabilities": {
                    "streaming": true,
                    "pushNotifications": false,
                    "stateTransitionHistory": true
                  },
                  "authentication": {"schemes": ["Bearer"]},
                  "defaultInputModes": ["application/pdf", "text/plain"],
                  "defaultOutputModes": ["application/json"],
                  "skills": [{
                    "id": "extract",
                    "name": "Extract invoice",
                    "description": "Parse invoice fields from documents",
                    "tags": ["ap"],
                    "examples": ["Extract totals from this PDF"]
                  }]
                }
        - title: Send a task
          description: >-
            Your agent sends a task with messages. The remote agent
            processes it and returns artifacts.
          actors_visible: [client, invoice-agent]
          messages:
            - from: client
              to: invoice-agent
              label: Submit task
              json_preview: '{"task":{"id":"t-123","state":"submitted"},"messages":[{"role":"user","parts":[...]}]}'
              json_full: |
                {
                  "task": {
                    "id": "t-123",
                    "state": "submitted"
                  },
                  "messages": [{
                    "role": "user",
                    "parts": [{
                      "content_type": "text/plain",
                      "content": "Extract invoice fields from https://example.com/inv.pdf"
                    }]
                  }]
                }
            - from: invoice-agent
              to: client
              label: Task artifacts
              json_preview: '{"task":{"state":"completed"},"artifacts":[{"parts":[{"content":"{...}"}]}]}'
        - title: Different agent, same protocol
          description: >-
            Your agent discovers a Research Agent at a different
            organization. Same AgentCard format, same task protocol.
          actors_visible: [client, research-agent]
          sparkle: true
          messages:
            - from: client
              to: research-agent
              label: Fetch AgentCard
              json_preview: 'GET /.well-known/agent-card.json'
            - from: research-agent
              to: client
              label: Capabilities
              json_preview: '{"name":"ResearchAgent","skills":[{"id":"search","name":"Deep research"}]}'

  # ── Layer 7: Domain Protocols ──────────────────────────────────
  - id: ucp
    title: UCP
    icon: images/icon-ucp.png
    icon_alt: "$"
    tagline: Universal agent commerce
    tree:
      col: 4
      row: 4
      depends_on: [a2a]
    detail:
      what_it_solves: >-
        Agents that shop need a standard way to browse products, build
        carts, and complete checkout across different merchants — without
        custom integration per store. UCP standardizes the entire commerce
        flow from discovery through post-purchase.
      why_open: >-
        Any agent can shop at any UCP merchant. Any merchant can serve any
        agent. One protocol covers the full journey without platform
        lock-in.
      where_from: >-
        Developed by Shopify and co-developed with Google. Launched with
        support from large retailers and payments ecosystem partners.
      who_maintains: >-
        Shopify and Google maintain the specification. Open for any
        merchant, agent, or payment processor to implement.
    animation:
      actors:
        - id: agent
          label: Shopping Agent
          type: agent
        - id: merchant-a
          label: Flower Shop
          type: merchant
        - id: merchant-b
          label: Book Store
          type: merchant
      scenes:
        - title: Agent creates a checkout
          description: >-
            The shopping agent creates a checkout session with a
            merchant, sending line items and buyer info.
          actors_visible: [agent, merchant-a]
          messages:
            - from: agent
              to: merchant-a
              label: Create checkout session
              json_preview: '{"line_items":[{"item":{"id":"bouquet_roses"},"quantity":1}],"currency":"USD"}'
              json_full: |
                POST /checkout-sessions
                UCP-Agent: profile="https://agent.example/profile"
                {
                  "line_items": [{
                    "item": {"id": "bouquet_roses", "title": "Red Rose Bouquet"},
                    "quantity": 1
                  }],
                  "buyer": {
                    "full_name": "John Doe",
                    "email": "john.doe@example.com"
                  },
                  "currency": "USD",
                  "payment": {
                    "instruments": [],
                    "handlers": [{
                      "id": "shop_pay",
                      "name": "com.shopify.shop_pay",
                      "version": "2026-01-11"
                    }]
                  }
                }
            - from: merchant-a
              to: agent
              label: Session ready
              json_preview: '{"id":"session-abc","status":"ready_for_complete","total":"$49.99"}'
              json_full: |
                {
                  "ucp": {
                    "version": "2026-01-11",
                    "capabilities": [{
                      "name": "dev.ucp.shopping.checkout",
                      "version": "2026-01-11"
                    }]
                  },
                  "id": "session-abc",
                  "status": "ready_for_complete",
                  "total": {"amount": 4999, "currency": "USD"}
                }
        - title: Agent completes purchase
          description: >-
            The agent confirms the purchase. The merchant processes
            payment and returns an order confirmation.
          actors_visible: [agent, merchant-a]
          messages:
            - from: agent
              to: merchant-a
              label: Complete checkout
              json_preview: '{"session_id":"session-abc","action":"complete"}'
            - from: merchant-a
              to: agent
              label: Order confirmed
              json_preview: '{"order_id":"ord-789","status":"confirmed"}'
        - title: Same protocol, different merchant
          description: >-
            The agent shops at a completely different store. Same
            checkout flow, same API shape — no new integration needed.
          actors_visible: [agent, merchant-b]
          sparkle: true
          messages:
            - from: agent
              to: merchant-b
              label: Create checkout session
              json_preview: '{"line_items":[{"item":{"id":"novel_123"},"quantity":2}],"currency":"USD"}'
            - from: merchant-b
              to: agent
              label: Session ready
              json_preview: '{"id":"session-xyz","status":"ready_for_complete","total":"$31.98"}'

  - id: agent-identity
    title: Agent Identity
    icon: images/icon-agent-identity.png
    icon_alt: "id"
    tagline: Verified agents on the open web
    tree:
      col: 4
      row: 5
      depends_on: [a2a]
    detail:
      what_it_solves: >-
        Websites and merchants need to distinguish legitimate,
        user-authorized agents from scrapers and bots. Agent Identity
        protocols use cryptographic signatures to prove an agent's
        identity on every request — no CAPTCHAs, no IP allowlists.
      why_open: >-
        Built on IETF HTTP Message Signatures — an open standard. Any
        agent can sign requests, any website can verify them. Visa TAP
        and Cloudflare Web Bot Auth both use the same foundation.
      where_from: >-
        Cloudflare proposed Web Bot Auth based on active IETF drafts.
        Visa built the Trusted Agent Protocol (TAP) on the same HTTP
        Message Signatures foundation.
      who_maintains: >-
        IETF maintains the HTTP Message Signatures standard. Cloudflare
        and Visa maintain their specific implementations (Web Bot Auth
        and TAP respectively).
    animation:
      actors:
        - id: agent
          label: Agent
          type: agent
        - id: merchant
          label: Merchant Site
          type: merchant
        - id: registry
          label: Key Registry
          type: registry
      scenes:
        - title: Agent signs its request
          description: >-
            The agent makes an HTTP request and attaches a
            cryptographic signature using HTTP Message Signatures.
          actors_visible: [agent, merchant]
          messages:
            - from: agent
              to: merchant
              label: Signed request
              json_preview: 'Signature-Input: sig2=("@authority" "@path");keyid="...";tag="agent-browser-auth"'
              json_full: |
                GET /example-product HTTP/1.1
                Host: example.com
                Signature-Input: sig2=("@authority" "@path");created=1735689600;
                  expires=1735693200;keyid="agent-key-123";
                  alg="Ed25519";nonce="abc123";
                  tag="agent-browser-auth"
                Signature: sig2=:BASE64_SIGNATURE_HERE:
        - title: Merchant verifies identity
          description: >-
            The merchant retrieves the agent's public key and verifies
            the signature. Legitimate agents get through; unknown bots don't.
          actors_visible: [agent, merchant, registry]
          messages:
            - from: merchant
              to: registry
              label: Fetch public key
              json_preview: 'GET /.well-known/agent-keys/agent-key-123'
            - from: registry
              to: merchant
              label: Public key
              json_preview: '{"keyid":"agent-key-123","alg":"Ed25519","public_key":"..."}'
            - from: merchant
              to: agent
              label: Access granted
              json_preview: 'HTTP/1.1 200 OK'
        - title: Same signatures work everywhere
          description: >-
            Whether it's Visa TAP, Cloudflare Web Bot Auth, or
            Mastercard Agent Pay — the same HTTP Message Signatures
            standard works across all of them.
          actors_visible: [agent, merchant]
          sparkle: true
          messages:
            - from: agent
              to: merchant
              label: 'Signed: tag="web-bot-auth"'
              json_preview: 'Signature-Input: sig2=("@authority" "@path");tag="web-bot-auth"'
            - from: merchant
              to: agent
              label: Verified
              json_preview: 'HTTP/1.1 200 OK — Agent identity confirmed'
